# EGOS Testing Architecture â€” Agents Testing Agents

> **VERSION:** 1.0.0 | **Created:** 2026-02-18
> **Philosophy:** Real tests close to production. AI computational power for verification. Agents that test, improve, and orchestrate each other.

---

## Why This Exists

Traditional unit tests become useless over time. They test implementation details, not behavior. They mock away the real complexity. They give false confidence.

**EGOS testing is different:**
- **Real endpoints** â€” hit actual API routes, not mocks
- **Real database** â€” query Supabase with real RLS policies
- **AI-powered** â€” use Gemini to generate edge cases, verify response quality, detect regressions
- **Multi-layer** â€” 5 layers of agents, each verifying different aspects
- **Self-improving** â€” agents review each other's findings, reduce false positives

---

## Architecture: 5 Testing Layers

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 TEST ORCHESTRATOR                     â”‚
â”‚    Runs all layers â†’ Combined report â†’ Tracks time   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                       â”‚
â”‚  Layer 5: AI VERIFIER                                â”‚
â”‚  â”œâ”€ AI evaluates AI response quality                 â”‚
â”‚  â”œâ”€ Generates adversarial edge-case inputs           â”‚
â”‚  â”œâ”€ Reviews other agents' findings (false positive?) â”‚
â”‚  â””â”€ Scores: accuracy, safety, consistency            â”‚
â”‚                                                       â”‚
â”‚  Layer 4: REGRESSION WATCHER                         â”‚
â”‚  â”œâ”€ Compares current results vs last N runs          â”‚
â”‚  â”œâ”€ Detects: new failures, flaky tests, degradation  â”‚
â”‚  â”œâ”€ Alerts on breaking changes                       â”‚
â”‚  â””â”€ Tracks: test health score over time              â”‚
â”‚                                                       â”‚
â”‚  Layer 3: INTEGRATION TESTER                         â”‚
â”‚  â”œâ”€ Full flow: request â†’ processing â†’ DB â†’ response  â”‚
â”‚  â”œâ”€ Supabase RLS: can user X access row Y?           â”‚
â”‚  â”œâ”€ Auth flows: token â†’ session â†’ protected route    â”‚
â”‚  â””â”€ Cross-service: API â†’ DB â†’ webhook â†’ response     â”‚
â”‚                                                       â”‚
â”‚  Layer 2: CONTRACT TESTER                            â”‚
â”‚  â”œâ”€ API routes: status codes, content types, schemas â”‚
â”‚  â”œâ”€ Request validation: missing fields, bad types    â”‚
â”‚  â”œâ”€ Error handling: 4xx, 5xx, rate limits            â”‚
â”‚  â””â”€ Response structure: matches TypeScript types     â”‚
â”‚                                                       â”‚
â”‚  Layer 1: STATIC ANALYSIS (existing agents)          â”‚
â”‚  â”œâ”€ SSOT Auditor: type consistency                   â”‚
â”‚  â”œâ”€ Dead Code: unused exports                        â”‚
â”‚  â”œâ”€ Dep Auditor: version conflicts                   â”‚
â”‚  â””â”€ Security Scanner: secrets, PII                   â”‚
â”‚                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Agent Registry (New Testing Agents)

| Agent ID | Layer | What It Tests | Risk |
|----------|-------|---------------|------|
| `contract_tester` | 2 | API schemas, status codes, error handling | low |
| `integration_tester` | 3 | Full flows, RLS, auth, cross-service | medium |
| `regression_watcher` | 4 | Historical comparison, flaky detection | low |
| `ai_verifier` | 5 | AI response quality, edge cases, false positives | medium |

---

## Layer 2: Contract Tester

Tests every API route for contract compliance:

```typescript
// For each API route:
// 1. Valid request â†’ 200 + correct response shape
// 2. Missing required field â†’ 400
// 3. Wrong method â†’ 405
// 4. No auth (if required) â†’ 401
// 5. Rate limit exceeded â†’ 429
// 6. Response headers correct (Content-Type, CORS)
```

**Targets (egos-web):**
- `POST /api/chat` â€” message â†’ AI response
- `GET /api/github-commits` â€” fetch recent commits
- `POST /api/ingest-commits` â€” analyze commits with AI

**Targets (Supabase direct):**
- `nexusmkt_products` â€” CRUD with RLS
- `eagle_eye_gazettes` â€” read-only public access
- `commits` â€” insert from ingestion
- `telemetry_events` â€” write-only from services

---

## Layer 3: Integration Tester

Tests complete flows:

```
Flow 1: Chat
  Input: { message: "What is EGOS?" }
  Expected: 200, response.reply exists, length > 50, no hallucination

Flow 2: Commit Ingestion
  Input: GitHub commits JSON
  Expected: 200, commits stored in DB, AI analysis generated

Flow 3: RLS Enforcement
  As anon: SELECT nexusmkt_products â†’ rows (public read)
  As anon: INSERT nexusmkt_products â†’ DENIED
  As authenticated: SELECT own orders â†’ rows
  As authenticated: SELECT other's orders â†’ empty

Flow 4: Supabase Functions
  Test edge functions respond correctly
  Test webhook endpoints accept valid payloads
```

---

## Layer 4: Regression Watcher

```
1. Load previous test results from agents/.logs/test-history.jsonl
2. Run current test suite
3. Compare:
   - Tests that PASSED before but FAIL now â†’ REGRESSION
   - Tests that FAIL intermittently â†’ FLAKY
   - New failures in previously untested areas â†’ NEW_ISSUE
   - Tests that FAILED before but PASS now â†’ FIXED
4. Generate regression report with diff
```

---

## Layer 5: AI Verifier

Uses AI to test AI:

```
1. Generate adversarial inputs:
   - SQL injection attempts in chat
   - Prompt injection ("ignore previous instructions")
   - Extremely long messages
   - Unicode edge cases
   - Empty/null inputs

2. Evaluate response quality:
   - Is the chat response factually correct about EGOS?
   - Does it hallucinate features that don't exist?
   - Is it safe (no PII, no harmful content)?
   - Is the tone appropriate?

3. Cross-verify agent findings:
   - Take SSOT Auditor findings â†’ Are they real issues?
   - Take Dead Code findings â†’ Would removing them break anything?
   - Score: precision, recall, actionability
```

---

## Execution Model

```bash
# Run all test layers
bun agent:test              # dry-run (plan what would be tested)
bun agent:test --exec       # execute (run real tests)
bun agent:test --layer 2    # run specific layer only
bun agent:test --report     # generate full test report

# Run individual test agents
bun agent:contract          # contract tests only
bun agent:integration       # integration tests only  
bun agent:regression        # regression analysis only
bun agent:ai-verify         # AI verification only
```

---

## Test Report Format

```markdown
# EGOS Test Report â€” 2026-02-18T14:00:00Z

## Summary
- Total tests: 47
- Passed: 43 (91.5%)
- Failed: 2
- Skipped: 2
- Duration: 12.4s

## By Layer
| Layer | Tests | Pass | Fail | Duration |
|-------|-------|------|------|----------|
| Contract | 15 | 14 | 1 | 2.1s |
| Integration | 12 | 11 | 1 | 8.2s |
| Regression | 10 | 10 | 0 | 0.3s |
| AI Verify | 10 | 8 | 0 | 1.8s |

## Failures
### FAIL: POST /api/chat â€” missing message field
- Expected: 400
- Actual: 500 (unhandled error)
- Fix: Add input validation before AI call

## Regressions
- None detected (baseline: previous run)

## AI Verification
- Chat quality score: 8.7/10
- Prompt injection resistance: 10/10
- Hallucination rate: 0% (5 factual checks)
```

---

## Implementation Order

1. âœ… Static Analysis agents (Layer 1) â€” already exist
2. ðŸ”¨ Contract Tester (Layer 2) â€” implement NOW
3. ðŸ”¨ Integration Tester (Layer 3) â€” implement NOW  
4. ðŸ“‹ Regression Watcher (Layer 4) â€” after baseline exists
5. ðŸ“‹ AI Verifier (Layer 5) â€” after Layers 2-3 produce data
6. ðŸ“‹ Test Orchestrator â€” wire into existing orchestrator
